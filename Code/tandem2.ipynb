{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f65eb16",
   "metadata": {
    "id": "7f65eb16"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras.models import Sequential\n",
    "from keras.models import Model\n",
    "from keras.layers import Dense, Dropout, InputLayer, Input, Flatten, Conv1D, MaxPooling1D, BatchNormalization\n",
    "import optuna\n",
    "import openpyxl\n",
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import shuffle\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.losses import SparseCategoricalCrossentropy\n",
    "from keras.losses import MeanAbsoluteError,mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3499434d-99a3-40f4-a476-06d028f504a5",
   "metadata": {
    "id": "3499434d-99a3-40f4-a476-06d028f504a5"
   },
   "outputs": [],
   "source": [
    "ds = pd.read_csv('intraindata2n.csv', header=None)\n",
    "ds = ds.sample(frac=1).reset_index(drop=True)\n",
    "x_train = np.array(ds.iloc[:, 5:])\n",
    "y1_train = np.array(ds[[0]])\n",
    "y2_train = np.array(ds[[1]])\n",
    "y3_train = np.array(ds[[2]])\n",
    "y4_train = np.array(ds[[3]])\n",
    "y5_train = np.array(ds[[4]])\n",
    "ds = pd.read_csv('invalidation2n.csv', header=None)\n",
    "ds = ds.sample(frac=1).reset_index(drop=True)\n",
    "x_val = np.array(ds.iloc[:, 5:])\n",
    "y1_val = np.array(ds[[0]])\n",
    "y2_val = np.array(ds[[1]])\n",
    "y3_val = np.array(ds[[2]])\n",
    "y4_val = np.array(ds[[3]])\n",
    "y5_val = np.array(ds[[4]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bf85e1a-2d9a-4bb5-a9a2-b78306f3efe4",
   "metadata": {
    "id": "4bf85e1a-2d9a-4bb5-a9a2-b78306f3efe4"
   },
   "outputs": [],
   "source": [
    "miny3 = np.min(y3_train)\n",
    "maxy3 = np.max(y3_train)\n",
    "miny4 = np.min(y4_train)\n",
    "maxy4 = np.max(y4_train)\n",
    "miny5 = np.min(y5_train)\n",
    "maxy5 = np.max(y5_train)\n",
    "miny31 = np.min(y3_val)\n",
    "maxy31= np.max(y3_val)\n",
    "miny41 = np.min(y4_val)\n",
    "maxy41 = np.max(y4_val)\n",
    "miny51 = np.min(y5_val)\n",
    "maxy51 = np.max(y5_val)\n",
    "y3_train = (y3_train - miny3) / (maxy3 - miny3)\n",
    "y4_train = (y4_train - miny4) / (maxy4 - miny4)\n",
    "y5_train = (y5_train - miny5) / (maxy5 - miny5)\n",
    "\n",
    "\n",
    "\n",
    "y3_val = (y3_val- miny31) / (maxy31 - miny31)\n",
    "y4_val = (y4_val - miny41) / (maxy41- miny41)\n",
    "y5_val = (y5_val - miny51) / (maxy51 - miny51)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efecb9cc",
   "metadata": {
    "id": "efecb9cc"
   },
   "outputs": [],
   "source": [
    "forward_model = keras.models.load_model('forward3.h5')\n",
    "forward_model.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c95ab14b",
   "metadata": {
    "id": "c95ab14b"
   },
   "outputs": [],
   "source": [
    "x1=Input(shape=x_train[0].shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20c9fe33-b92d-42ff-9099-3094c97d4f3c",
   "metadata": {
    "id": "20c9fe33-b92d-42ff-9099-3094c97d4f3c"
   },
   "outputs": [],
   "source": [
    "in1 = Input(shape=x_train[0].shape)\n",
    "\n",
    "x1 = Dense(500, activation='relu')(in1)\n",
    "x1 = Dense(500, activation='relu')(x1)\n",
    "x1 = Dense(500, activation='relu')(x1)\n",
    "x1 = Dense(500, activation='relu')(x1)\n",
    "out1 = Dense(2, activation='softmax')(x1)\n",
    "\n",
    "x2 = Dense(800, activation='relu')(in1)\n",
    "x2 = Dense(800, activation='relu')(x2)\n",
    "x2 = Dense(800, activation='relu')(x2)\n",
    "x2 = Dense(800, activation='relu')(x2)\n",
    "out2 = Dense(3, activation='softmax')(x2)\n",
    "\n",
    "xa = Dense(1180, activation='relu')(in1)\n",
    "xa = Dense(1180, activation='relu')(xa)\n",
    "xa = Dense(1180, activation='relu')(xa)\n",
    "xa = Dense(1180, activation='relu')(xa)\n",
    "xa = Dense(1180, activation='relu')(xa)\n",
    "xa = Dense(1180, activation='relu')(xa)\n",
    "out3 = Dense(1, activation='linear')(xa)\n",
    "xb = Dense(1180, activation='relu')(in1)\n",
    "xb = Dense(1180, activation='relu')(xb)\n",
    "xb = Dense(1180, activation='relu')(xb)\n",
    "xb = Dense(1180, activation='relu')(xb)\n",
    "xb = Dense(1180, activation='relu')(xb)\n",
    "xb = Dense(1180, activation='relu')(xb)\n",
    "out4 = Dense(1, activation='linear')(xb)\n",
    "\n",
    "xc = Dense(1180, activation='relu')(in1)\n",
    "xc = Dense(1180, activation='relu')(xc)\n",
    "xc = Dense(1180, activation='relu')(xc)\n",
    "xc = Dense(1180, activation='relu')(xc)\n",
    "xc = Dense(1180, activation='relu')(xc)\n",
    "xc = Dense(1180, activation='relu')(xc)\n",
    "out5 = Dense(1, activation='linear')(xc)\n",
    "inverse_model = Model(inputs=in1, outputs=(out1, out2, out3, out4, out5))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cc837bf",
   "metadata": {
    "id": "5cc837bf"
   },
   "outputs": [],
   "source": [
    "def merge(out1, out2, out3, out4, out5):\n",
    "    argmax=tf.math.argmax(out2, axis=1)\n",
    "    arg=tf.expand_dims(argmax, axis=1)\n",
    "\n",
    "    argmax1=tf.math.argmax(out1, axis=1)\n",
    "    arg1=tf.expand_dims(argmax1, axis=1)\n",
    "\n",
    "    outf1 = tf.cast(arg1, dtype=tf.float32)\n",
    "    outf2 = tf.cast(arg, dtype=tf.float32)\n",
    "    merge=tf.concat([outf1, outf2, out3, out4, out5], axis=1)\n",
    "    return merge\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ebdf94c",
   "metadata": {
    "id": "4ebdf94c"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bf337cb",
   "metadata": {
    "id": "2bf337cb",
    "outputId": "88842f6c-4ad8-4974-873e-834465b96472"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(None, 5)\n",
      "y1_train dtype: <dtype: 'float32'>\n",
      "y2_train dtype: <dtype: 'float32'>\n"
     ]
    }
   ],
   "source": [
    "in1 = Input(shape=x_train[0].shape)\n",
    "inverse_out=inverse_model(in1)\n",
    "ou1 = inverse_out[0]\n",
    "ou2 = inverse_out[1]\n",
    "ou3 = inverse_out[2]\n",
    "ou4 = inverse_out[3]\n",
    "ou5 = inverse_out[4]\n",
    "outf=merge(ou1, ou2, ou3, ou4, ou5)\n",
    "print(outf.shape)\n",
    "interout=forward_model(outf)\n",
    "tandem_out=interout\n",
    "tandem = keras.models.Model(inputs=in1, outputs=(tandem_out, ou1, ou2, ou3, ou4, ou5))\n",
    "\n",
    "print(\"y1_train dtype:\", ou1.dtype)\n",
    "print(\"y2_train dtype:\", ou2.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77d2b891-3d89-4ba3-b949-aab2a76406a0",
   "metadata": {
    "id": "77d2b891-3d89-4ba3-b949-aab2a76406a0"
   },
   "outputs": [],
   "source": [
    "optimizer=tf.keras.optimizers.Adam(0.000325)\n",
    "tandem.compile(optimizer=optimizer, loss=['mse','SparseCategoricalCrossentropy', 'SparseCategoricalCrossentropy', 'mse', 'mse', 'mse'],\n",
    "              metrics=[['mse'], ['accuracy'], ['accuracy'], ['mse','mae'], ['mse','mae'], ['mse','mae']])\n",
    "early=EarlyStopping(monitor='val_loss', patience=100, mode='min', restore_best_weights=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28399916-8f4e-43aa-849f-6f1f8271e470",
   "metadata": {
    "id": "28399916-8f4e-43aa-849f-6f1f8271e470"
   },
   "outputs": [],
   "source": [
    "history = tandem.fit(x_train, (x_train, y1_train, y2_train, y3_train, y4_train, y5_train),validation_data=(x_val,[x_val, y1_val, y2_val, y3_val, y4_val, y5_val]),\n",
    "\n",
    "                     batch_size=128, epochs=1000, validation_batch_size=128, verbose=2, shuffle=True, callbacks=[ early]\n",
    "                     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c583926e",
   "metadata": {
    "id": "c583926e"
   },
   "outputs": [],
   "source": [
    "val_loss = history.history['loss']\n",
    "df1=pd.DataFrame(val_loss)\n",
    "df1.to_excel('tanloss3.xlsx')\n",
    "loss=history.history['sequential_loss']\n",
    "df2=pd.DataFrame(loss)\n",
    "df2.to_excel('tanseqloss3.xlsx')\n",
    "val_loss = history.history['val_loss']\n",
    "df3=pd.DataFrame(val_loss)\n",
    "df3.to_excel('tanvalidation_loss3.xlsx')\n",
    "loss=history.history['val_sequential_loss']\n",
    "df4=pd.DataFrame(loss)\n",
    "df4.to_excel('tanvalseqloss3.xlsx')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82077625",
   "metadata": {
    "id": "82077625",
    "outputId": "81529b7d-8b05-45f5-df0a-e85d77044da4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    }
   ],
   "source": [
    "tandem.save('models/inversenet_tandem_full3.h5')\n",
    "inverse_model.save('inversenet_tandem_generator3.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dda41596",
   "metadata": {
    "id": "dda41596"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
