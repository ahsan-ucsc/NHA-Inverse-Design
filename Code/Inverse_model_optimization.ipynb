{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2559679a",
   "metadata": {
    "id": "2559679a",
    "outputId": "9a328c61-ff58-491d-9698-56ff42870aa3"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\Anaconda\\envs\\tfnew\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras.models import Sequential\n",
    "from keras.models import Model\n",
    "from keras.layers import Dense, Dropout, InputLayer, Input, Flatten, Conv1D, MaxPooling1D\n",
    "import optuna\n",
    "import openpyxl\n",
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import shuffle\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.losses import SparseCategoricalCrossentropy\n",
    "from keras.losses import MeanAbsoluteError\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeaa8971",
   "metadata": {
    "id": "eeaa8971"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45640bc7",
   "metadata": {
    "id": "45640bc7"
   },
   "outputs": [],
   "source": [
    "ds = pd.read_csv('intraindata2n.csv', header=None)\n",
    "ds = ds.sample(frac=1).reset_index(drop=True)\n",
    "x_train = np.array(ds.iloc[:, 5:])\n",
    "y1_train = np.array(ds[[0]])\n",
    "y2_train = np.array(ds[[1]])\n",
    "y3_train = np.array(ds[[2]])\n",
    "y4_train = np.array(ds[[3]])\n",
    "y5_train = np.array(ds[[4]])\n",
    "ds = pd.read_csv('intestdata2.csv', header=None)\n",
    "ds = ds.sample(frac=1).reset_index(drop=True)\n",
    "x_test = np.array(ds.iloc[:, 5:])\n",
    "y1_test = np.array(ds[[0]])\n",
    "y2_test = np.array(ds[[1]])\n",
    "y3_test = np.array(ds[[2]])\n",
    "y4_test = np.array(ds[[3]])\n",
    "y5_test = np.array(ds[[4]])\n",
    "ds = pd.read_csv('invalidation2n.csv', header=None)\n",
    "ds = ds.sample(frac=1).reset_index(drop=True)\n",
    "x_val = np.array(ds.iloc[:, 5:])\n",
    "y1_val = np.array(ds[[0]])\n",
    "y2_val = np.array(ds[[1]])\n",
    "y3_val = np.array(ds[[2]])\n",
    "y4_val = np.array(ds[[3]])\n",
    "y5_val = np.array(ds[[4]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71101450",
   "metadata": {
    "id": "71101450"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "196f1a73",
   "metadata": {
    "id": "196f1a73"
   },
   "outputs": [],
   "source": [
    "miny3 = np.min(y3_train)\n",
    "maxy3 = np.max(y3_train)\n",
    "miny4 = np.min(y4_train)\n",
    "maxy4 = np.max(y4_train)\n",
    "miny5 = np.min(y5_train)\n",
    "maxy5 = np.max(y5_train)\n",
    "y3_train = (y3_train - miny3) / (maxy3 - miny3)\n",
    "y4_train = (y4_train - miny4) / (maxy4 - miny4)\n",
    "y5_train = (y5_train - miny5) / (maxy5 - miny5)\n",
    "\n",
    "y3_test = (y3_test - miny3) / (maxy3 - miny3)\n",
    "y4_test = (y4_test - miny4) / (maxy4 - miny4)\n",
    "y5_test = (y5_test - miny5) / (maxy5 - miny5)\n",
    "\n",
    "y3_val = (y3_val- miny3) / (maxy3 - miny3)\n",
    "y4_val = (y4_val - miny4) / (maxy4 - miny4)\n",
    "y5_val = (y5_val - miny5) / (maxy5 - miny5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b83191c",
   "metadata": {
    "id": "1b83191c"
   },
   "outputs": [],
   "source": [
    "def objective(trial):\n",
    "    in1 = Input(shape=x_train[0].shape)\n",
    "\n",
    "    x = Dense(500, activation='relu')(in1)\n",
    "    x = Dense(500, activation='relu')(x)\n",
    "    x = Dense(500, activation='relu')(x)\n",
    "    x = Dense(500, activation='relu')(x)\n",
    "    out1 = Dense(2, activation='softmax')(x)\n",
    "\n",
    "    x = Dense(800, activation='relu')(in1)\n",
    "    x = Dense(800, activation='relu')(x)\n",
    "    x = Dense(800, activation='relu')(x)\n",
    "    x = Dense(800, activation='relu')(x)\n",
    "    out2 = Dense(3, activation='softmax')(x)\n",
    "\n",
    "    num_layers_m1=trial.suggest_int('num_layers_m1' , 1,10 )\n",
    "    num_neurons_m1=trial.suggest_int('num_neurons_m1' ,100, 2000)\n",
    "    lr=trial.suggest_float('lr', 0.0001, 0.001, log=True)\n",
    "\n",
    "\n",
    "    x_p1=Dense(num_neurons_m1, activation='relu')(in1)\n",
    "    for i in range(num_layers_m1):\n",
    "        x_p1=Dense(num_neurons_m1, activation='relu')(x_p1)\n",
    "    x_p1=Dense(1, activation='linear')(x_p1)\n",
    "\n",
    "    x_p2=Dense(num_neurons_m1, activation='relu')(in1)\n",
    "    for i in range(num_layers_m1):\n",
    "        x_p2=Dense(num_neurons_m1, activation='relu')(x_p2)\n",
    "    x_p2=Dense(1, activation='linear')(x_p2)\n",
    "\n",
    "    x_p3=Dense(num_neurons_m1, activation='relu')(in1)\n",
    "    for i in range(num_layers_m1):\n",
    "        x_p3=Dense(num_neurons_m1, activation='relu')(x_p3)\n",
    "    x_p3=Dense(1, activation='linear')(x_p3)\n",
    "    model=Model(inputs=in1, outputs=[ out1, out2, x_p1, x_p2, x_p3])\n",
    "\n",
    "    optimizer=tf.keras.optimizers.Adam(lr)\n",
    "    model.compile(optimizer=optimizer, loss=['SparseCategoricalCrossentropy', 'SparseCategoricalCrossentropy','mse', 'mse', 'mse'],\n",
    "              metrics=[['accuracy'], ['accuracy'],['mse'], ['mse'], ['mse']])\n",
    "    early=EarlyStopping(monitor='val_loss', patience=10, mode='min', restore_best_weights=True)\n",
    "\n",
    "    model.fit(x_train,( y1_train, y2_train, y3_train, y4_train, y5_train), validation_data=(x_val,[y1_val, y2_val, y3_val, y4_val, y5_val]), batch_size=128, epochs=500, verbose=0,validation_batch_size=128, shuffle=True, callbacks=[early])\n",
    "\n",
    "    score=model.evaluate(x_test, [y1_test, y2_test, y3_test, y4_test, y5_test])\n",
    "    return score[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d205bff",
   "metadata": {
    "id": "9d205bff"
   },
   "outputs": [],
   "source": [
    "study= optuna.create_study(direction=\"minimize\")\n",
    "study.optimize(objective, n_trials=100)\n",
    "print(\"Number of finished trials: {}\".format(len(study.trials)))\n",
    "print(\"Best trial:\")\n",
    "trial= study.best_trial\n",
    "print(\" Value: {}\".format(trial.value))\n",
    "print(\"  Params  \")\n",
    "for key, value in trial.params.items():\n",
    "    print(\"  {}: {}\".format(key, value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "259e6664",
   "metadata": {
    "id": "259e6664"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
